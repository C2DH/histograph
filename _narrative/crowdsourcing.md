---
ordering: 2
---
## Crowd-sourced indexation

###User input
Three different systems are in place to collect user input: Questions on the overall validity of an *entity* (“Is this a person?”), questions on the validity of an *entity annotation* in an object (“Is this person mentioned here?”) and *personalised notifications* based and previous actions of a user (“User x added a person to a document you worked on. Can you confirm this annotation?”). 

###Confirmation and Annotation 
All annotations can be in one of three stages: *not validated, validated or disputed*. In addition, users are encouraged to fix mistakes themselves by annotating new entities and by flagging wrong entity types, fragments, duplicates or erroneous annotations. To avoid accidental annotations and reduce the risk of vandalism, HistoGraph treats every annotation as a suggestion pending confirmation by other users.

###Generic and expert crowds
We operate with two types of crowd task: tasks targeted at a *generic crowd*, which means that anyone is able to provide input, and harder, more challenging tasks, which target *expert users*. Users qualify for these expert tasks on the basis of their previous actions. For example, a user who annotates many documents associated with Pierre Werner will be asked to validate related annotations by others and to identify unknown entities in related documents.
